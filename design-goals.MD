# Design Goals

## Updated Problem Statement
Users highly prioritize efficiency in their AI-powered workflows; in higher complexity workflows, this is a balance between minimizing iteration count, net time spent, and preserving transparency. The fulfillment of these qualities are not adequately served by popular market options, reducing user efficiency and willingness to engage with these systems.

## Paper Prototype
Note that we are pivoting, so the originally presented canvas view will be treated as a stretch goal.
![Paper Prototype for Pre-privot Idea]()
NOTE: left hand side represents the chat interface and the right hand side is the canvas/working copy of workflow goal.

## Key Takeaways
1. For people unfamiliar with the OpenAI Canvas UI, it was initially hard to figure out what was going on (left hand chat w/right hand side editor). They needed some guidance from the facilitator, and one individual indicated disapproval at the break from the chat-based workflow. A brief popup if we decide to continue with this idea would address this nicely.
2. Users generally appreciated the intuition and presence behind the dedicated interrupt button; however, there were some initial difficulties in terms of intuitively identifying its purpose with respect to the thinking process. Perhaps a brief tutorial or introduction to the interrupt button would solve this issue.
3. Users wanted to have more “intuitive” interactions with the canvas view in terms of querying: e.g. highlighting text to follow up on, rather than being forced to query through a new prompt; this is potentially an aspect to explore in terms of streamlining user interaction.

## Design Goals
With the recent unveiling of Anthropic’s Claude 3.7 Sonnet / Claude Code, we want to do a pivot in our methodology to fully exploit Claude’s parameterized thinking budget + extended thinking mode to deliver our design goals. While we initially converged on an enhanced version of an OpenAI Canvas or Claude Artifacts interface, where the initial draft of a document/code/etc would be generated by a lighter weight model to give the user some instant feedback, and they improved upon by the slower thinking model.

**Design Goal 1:**
- What is the goal?:
Decrease the raw iteration-count necessary during querying of user-LLM interactions, e.g. by reducing the necessity of back-and-forths to obtain satisfactory answers from the agent.
- Why is it important? How evidence (from your user research or related work) convinces you of its importance? Include any quotes, citations, etc. that help make your point:
Our user research indicated that complex workflows became prohibitively tedious and time-intensive due to the hand-holding necessary to guide the AI to the solution. E.g., the ECE major from our user research, and follow-up interviews with more complex workflows, have displayed iterative approaches to building AI context, adjusting it during errors, and scrutinizing closely the code produced. Hand-holding reflects a major qualm that users have with AI transparency. Furthermore, lacking answer quality (e.g. in regards to the depth, or feasibility analysis, or self-critical fact checking of LLM responses) often necessitates users to follow-up for a more satisfactory answer, or handhold the LLM’s context until it proves informed enough to deliver a satisfactory answer.
- How will you design for this goal? What implementation choice will you make?:
While not a perfect solution to the issue of iteration count, user research confirms that thinking models generally produce higher answer quality, while also producing answers beyond what was generally queried. Furthermore, these models are able to uphold the level of transparency and answer quality that users have indicated approval of. Due to its relative freshness on the market and its reported effectiveness, we choose to hook in Anthropic’s Claude 3.7 as the thinking model of choice for our (higher complexity?) query responses.

**Design Goal 2:**
- What is the goal?: 
Deterministically providing a ‘intelligence level’ or latency tailored to the complexity or requirements of the user’s use case. This should be easily customizable from the UI, with a provided heuristic metric in order to suggest a level of thinking depth based on the user’s message/task.
- Why is it important? How evidence (from your user research or related work) convinces you of its importance? Include any quotes, citations, etc. that help make your point:
A common reaction during the introductions many users had to R1’s thinking mechanism is: “Why is it yapping so much?” While many users eventually came to appreciate the level of detail and the unique quirks of R1’s thinking, a common sentiment expressed in our unstructured post-study interviews was the frustrating latency and long-windedness associated with this level of transparency. While this transparency is good for many use cases: other use cases, even complex, can be served without this level of detail. User determinism in setting ‘what level of detail’ is necessary, or adapting a system to proactively ascertain the level of detail required, would address this issue. User perception of ‘thoughtfulness’ and long-windedness was often closely tied to their use cases; for example, the AI-as-Google persona reacted extremely negatively to high latency, high thought answers, while the AI-as-Intern persona reacted with enthusiasm with respect to the answer quality and transparency. With the arrival of an API dedicated to tailoring the level of potential latency/thought associated with a query, we have an interesting opportunity to confirm if users’ individual experiences and use cases can be proactively served with varying levels of LLM thought.
- How will you design for this goal? What implementation choice will you make?:
Anthropic’s parameterized thinking budget allows us to proactively tailor responses, answer quality, and transparency to a user’s use case. To determine use cases and map them to thought budget parameters, we intend to integrate some NLP heuristic for determining query complexity and desired use case; for example, an extremely simplistic and convenient method would be to hook in another LLM to process these queries with Langchain, but we believe there’s much more efficient methods. In so doing, we provide an intelligence level that serves use cases with a high risk of LLM error, while also not compromising simpler use cases with needless latency; all without any overhead on the user’s end to optimally budget for their desired experience. We’re also going to experiment with a few different sliders/knobs along with complexity metric visualizations to see which one makes the most sense.

## Implementation Plan

Wednesday, Feb 26: Research potential heuristic ideas, Anthropic’s parameterized thought/response API, refresh on Streamlit documentation.
Thursday, Feb 27th - Friday, Feb 28th: Implement bulk of ideas, or at least backend application. This involves heuristic processing of queries and mapping to Anthropic’s API, with UI/UX components being a stretch goal for these days.
Saturday, March 1st - Sunday, March 2nd: Iron out final details; e.g. UI/UX components, streaming responses, etc.

## Contributions

We collectively worked to determine our updated problem statement after guidance from the in-class assignment, design goals, and implementation timeline. Each of us facilitated/observed a walkthrough. Michael Simon made the paper prototype. 

## Did you use a generative AI tool? If so, which and how?

Not this time!

## How much time did you spend on this assignment
- as a group? Michael: +15 min on the the paper prototype
- individually? 25 minutes each discussing the questions and our answers to each question. 

  



